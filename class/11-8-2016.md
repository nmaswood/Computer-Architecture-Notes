# Lecture 12 MultiCore

* Outline
    * Motivation
    * Challenges
* Paradigm Shift
    * before mid 2000s pushed for single performance
    * fast memmory access: on chip caches
    * exploiting instruction level parallelism
        * pipeline (Deep)
            * complex w/ dependencies
        * superscalar
        * out of order
        * simd
* Focus on task level parallelism
    * Multicore era
    * proliferation of cmp
* Why single core to multicore?
    * Power wall
        * more complexity -> more transistors -> more power
        * higher clock rate -> more switching -> power power
        * limited by cooling
    * no large benefits from ilp
        * diminishing returns
        * degrees of ilp is limited
        * pallock's rule: complexity of logic is proporitional to square of logic
* Multicore benefits
    * Performance
        * Latency
        * Throughput
    * Power
    * Complexity, yield, reliability
        * Yield is percentage of chips that passes all the tests you care about
    * Trade offs?
* Power Benefits
    * N units at frequency F/N consumes less power than 1 unit at F
    * Dynamic poewr modeled as a * C * V^2 * F
    * assume same workload, uarch, tech -> a * C is constant
    * Lower F -> Lower V (linear) -> cubic reduction in power
    * Leakage power is v important as well
* Task-Level Parallelsim
    * Different 'task/thread' executed in parallel
        * contrast with ilp, data parallism
    * How to create tasks
        * Partition a single problem in multiple related tasks
* Computers to Exploit Task Level Parallelism
    * loosely coupled
        * message passing / network
        * data center, multicomputer network
    * tightly coupled
        * shared memory address space
        * multi core, multi-thread
        * data sharing is implicit
            *  require synchronization
    * latency is the difference that matters
* Multicore
    * Cannot rely on compilers or hardware to improve performance like it as done in the past
    * Programmers must explicitly partition the problem into multiple related tasks
    * Programmer must explicitly partition the problem in mulitple related tasks (threads)
        * Different models
            * pthreads
            * open mp
    * Some programs easy to partition; others are more difficult
    * How to guarantee correctness
* Coordinating Access to Shared Data
    * Locks: simple primitive to ensure updates to single variable occur within a critical section
        * many variaitions
        * nobody can touch r1 unless I am done
* Performance vs Correctness
    * Few locks (coarse grain locking)
        * easy to write
        * poor preformance, a lot stalling, waiting
    * many locks
        * good performance
        * difficult to write
        * higher chance of incorrect
    * Consider trade offs wisely!
* Coodinating acess to shared data (II)
    * Split the operaton up
* Perf Analysis
    * Speed up with P cores = t_1 / t_p
    * linear speed up is the ideal case
    * sublinear speed up 
